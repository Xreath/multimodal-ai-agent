"""
Voice AI â€” ASR (Whisper) + TTS (Edge-TTS) modÃ¼lÃ¼.

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Voice AI Pipeline                                               â•‘
â•‘                                                                  â•‘
â•‘  ASR (Automatic Speech Recognition):                            â•‘
â•‘  Ses â†’ Whisper â†’ Metin                                          â•‘
â•‘                                                                  â•‘
â•‘  TTS (Text-to-Speech):                                          â•‘
â•‘  Metin â†’ Edge-TTS â†’ Ses dosyasÄ± (.mp3)                         â•‘
â•‘                                                                  â•‘
â•‘  Agent entegrasyonu:                                             â•‘
â•‘  KullanÄ±cÄ± ses â†’ ASR â†’ text query â†’ Agent â†’ text answer â†’ TTS  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Whisper Mimarisi (MÃ¼lakat notu):

1. Encoder-Decoder Transformer:
   - Encoder: Ses â†’ Mel spectrogram â†’ Transformer encoder â†’ audio features
   - Decoder: Audio features + Ã¶nceki token'lar â†’ sonraki token (autoregressive)

2. Mel Spectrogram nedir?
   - Ses dalgasÄ± â†’ STFT (Short-Time Fourier Transform) â†’ Spectrogram
   - Spectrogram â†’ Mel Ã¶lÃ§eÄŸi (insan kulaÄŸÄ±na uygun frekans daÄŸÄ±lÄ±mÄ±)
   - 80 mel filtre, 30 saniyelik pencereler
   - SonuÃ§: 2D "resim" â†’ CNN/Transformer ile iÅŸlenebilir

3. Whisper Model BoyutlarÄ±:
   | Model  | Parametre | VRAM  | HÄ±z (rel.) | WER (en) |
   |--------|-----------|-------|------------|----------|
   | tiny   | 39M       | ~1GB  | 32x        | ~7.6%    |
   | base   | 74M       | ~1GB  | 16x        | ~5.0%    |
   | small  | 244M      | ~2GB  | 6x         | ~3.4%    |
   | medium | 769M      | ~5GB  | 2x         | ~2.7%    |
   | large  | 1550M     | ~10GB | 1x         | ~2.1%    |

4. Whisper vs Alternatifler:
   | Ã–zellik         | Whisper      | Google STT   | Azure STT    |
   |-----------------|-------------|--------------|--------------|
   | Dil desteÄŸi     | 100+ dil    | 125+ dil     | 100+ dil     |
   | Offline         | âœ… Evet      | âŒ HayÄ±r     | âŒ HayÄ±r     |
   | Maliyet         | Ãœcretsiz    | $0.006/15s   | $0.016/dakika|
   | DoÄŸruluk (en)   | ~2-5% WER   | ~4-5% WER   | ~3-5% WER   |
   | Real-time       | Batch only  | Streaming âœ…  | Streaming âœ…  |

   â†’ Whisper: Offline, Ã¼cretsiz, batch processing iÃ§in ideal
   â†’ Google/Azure: Real-time streaming gerekiyorsa

5. Edge-TTS nedir?
   - Microsoft Edge'in TTS servisini kullanan Python kÃ¼tÃ¼phanesi
   - Ãœcretsiz (API key gerektirmez)
   - 300+ ses, 80+ dil (TÃ¼rkÃ§e dahil)
   - YÃ¼ksek kalite (neural TTS)
   - Alternatifler: OpenAI TTS ($15/1M char), Bark (offline, yavaÅŸ), gTTS (dÃ¼ÅŸÃ¼k kalite)
"""

import os
import asyncio
import tempfile
from typing import Optional


class WhisperASR:
    """
    Whisper ASR â€” Ses dosyasÄ±nÄ± metne Ã§evirir.

    KullanÄ±m:
        asr = WhisperASR(model_size="base")
        text = asr.transcribe("audio.wav")
        # â†’ "Bu sahnede kaÃ§ araÃ§ var?"

    MÃ¼lakat notu:
    - Whisper modeli lazy-loaded (ilk kullanÄ±mda indirilir)
    - fp16=True GPU'da hÄ±zlÄ± ama CPU'da False olmalÄ±
    - language="tr" TÃ¼rkÃ§e zorlama â€” auto-detect de yapabilir
    - Beam search (beam_size=5) â†’ daha doÄŸru ama daha yavaÅŸ
    """

    def __init__(self, model_size: str = "base"):
        """
        Args:
            model_size: "tiny", "base", "small", "medium", "large"
                        - tiny/base: HÄ±zlÄ±, demo iÃ§in yeterli
                        - small: Ä°yi denge (hÄ±z/doÄŸruluk)
                        - medium/large: En doÄŸru ama yavaÅŸ + Ã§ok VRAM
        """
        self.model_size = model_size
        self._model = None

    def _ensure_loaded(self):
        """Whisper modelini lazy load et."""
        if self._model is not None:
            return

        import whisper
        print(f"[Whisper] Loading '{self.model_size}' model...")
        self._model = whisper.load_model(self.model_size)
        print(f"[Whisper] Model loaded âœ…")

    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        task: str = "transcribe"
    ) -> dict:
        """
        Ses dosyasÄ±nÄ± metne Ã§evir.

        Args:
            audio_path: Ses dosyasÄ± yolu (.wav, .mp3, .m4a, .flac, vb.)
            language: Dil kodu ("tr", "en", vb.) â€” None â†’ auto-detect
            task: "transcribe" (aynÄ± dilde) veya "translate" (Ä°ngilizce'ye Ã§evir)

        Returns:
            {
                "text": "Tam transkript",
                "language": "tr",
                "segments": [{"start": 0.0, "end": 2.5, "text": "..."}],
                "duration": 5.3
            }

        MÃ¼lakat notu:
        - task="translate": Herhangi bir dilden Ä°ngilizce'ye Ã§evirme
          (Whisper'Ä±n Ã¶zel yeteneÄŸi â€” tek modelde hem ASR hem Ã§eviri)
        - segments: Zaman damgalÄ± Ã§Ä±ktÄ± â†’ altyazÄ±, video senkronizasyonu
        - fp16=False: CPU'da Ã§alÄ±ÅŸÄ±rken gerekli (MPS/CUDA'da True)
        """
        self._ensure_loaded()

        if not os.path.exists(audio_path):
            return {"error": f"Audio file not found: {audio_path}"}

        print(f"[Whisper] Transcribing: {audio_path}")

        # CPU mu GPU mu kontrol et
        import torch
        fp16 = torch.cuda.is_available()  # MPS'te de False olmalÄ±

        options = {
            "fp16": fp16,
            "task": task,
        }
        if language:
            options["language"] = language

        result = self._model.transcribe(audio_path, **options)

        # SÃ¼re hesapla
        segments = result.get("segments", [])
        duration = segments[-1]["end"] if segments else 0

        output = {
            "text": result["text"].strip(),
            "language": result.get("language", "unknown"),
            "segments": [
                {
                    "start": round(s["start"], 2),
                    "end": round(s["end"], 2),
                    "text": s["text"].strip()
                }
                for s in segments
            ],
            "duration": round(duration, 2)
        }

        print(f"[Whisper] Done â€” {output['language']}, {output['duration']}s, "
              f"{len(output['text'])} chars")

        return output

    def detect_language(self, audio_path: str) -> dict:
        """
        Ses dosyasÄ±nÄ±n dilini tespit et (transcribe yapmadan).

        Ä°lk 30 saniyeyi analiz eder.
        """
        self._ensure_loaded()
        import whisper

        audio = whisper.load_audio(audio_path)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(self._model.device)

        _, probs = self._model.detect_language(mel)
        top_langs = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]

        return {
            "detected_language": top_langs[0][0],
            "confidence": round(top_langs[0][1], 4),
            "top_5": {lang: round(prob, 4) for lang, prob in top_langs}
        }


class EdgeTTS:
    """
    Edge-TTS â€” Metni sese Ã§evirir (Microsoft Edge neural TTS).

    KullanÄ±m:
        tts = EdgeTTS()
        await tts.synthesize("Merhaba dÃ¼nya", "output.mp3")
        # veya senkron:
        tts.synthesize_sync("Merhaba dÃ¼nya", "output.mp3")

    MÃ¼lakat notu:
    - Edge-TTS Ã¼cretsiz ve API key gerektirmez
    - Neural TTS: doÄŸal ses kalitesi (eski concatenative TTS'ten Ã§ok daha iyi)
    - SSML desteÄŸi: konuÅŸma hÄ±zÄ±, tonlama, vurgu kontrol edilebilir
    - Async API: aiohttp ile Microsoft sunucularÄ±na baÄŸlanÄ±r
    """

    # PopÃ¼ler TÃ¼rkÃ§e ve Ä°ngilizce sesler
    VOICES = {
        "tr_female": "tr-TR-EmelNeural",
        "tr_male": "tr-TR-AhmetNeural",
        "en_female": "en-US-JennyNeural",
        "en_male": "en-US-GuyNeural",
        "en_aria": "en-US-AriaNeural",
    }

    def __init__(self, voice: str = "tr_female"):
        """
        Args:
            voice: Ses adÄ± â€” VOICES dict'indeki key veya doÄŸrudan voice ID
                   Ã–rnek: "tr_female", "en_male", "en-US-JennyNeural"
        """
        self.voice = self.VOICES.get(voice, voice)

    async def synthesize(
        self,
        text: str,
        output_path: str,
        rate: str = "+0%",
        volume: str = "+0%",
        pitch: str = "+0Hz"
    ) -> dict:
        """
        Metni sese Ã§evir (async).

        Args:
            text: Okunacak metin
            output_path: Ã‡Ä±ktÄ± dosya yolu (.mp3)
            rate: KonuÅŸma hÄ±zÄ± ("+20%" â†’ daha hÄ±zlÄ±, "-20%" â†’ daha yavaÅŸ)
            volume: Ses seviyesi
            pitch: Ses tonu

        Returns:
            {"output_path": "...", "voice": "...", "text_length": 42, "duration_estimate": 3.5}
        """
        import edge_tts

        communicate = edge_tts.Communicate(
            text=text,
            voice=self.voice,
            rate=rate,
            volume=volume,
            pitch=pitch
        )

        os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
        await communicate.save(output_path)

        # Tahmini sÃ¼re (ortalama 150 kelime/dk = 2.5 kelime/sn)
        word_count = len(text.split())
        duration_estimate = round(word_count / 2.5, 1)

        return {
            "output_path": output_path,
            "voice": self.voice,
            "text_length": len(text),
            "word_count": word_count,
            "duration_estimate_seconds": duration_estimate
        }

    def synthesize_sync(
        self,
        text: str,
        output_path: str,
        rate: str = "+0%",
        volume: str = "+0%",
        pitch: str = "+0Hz"
    ) -> dict:
        """
        Metni sese Ã§evir (senkron wrapper).

        asyncio.run() ile async fonksiyonu senkron Ã§aÄŸÄ±rÄ±r.
        Agent node'larÄ± senkron olduÄŸu iÃ§in bu wrapper gerekli.
        """
        # Event loop zaten Ã§alÄ±ÅŸÄ±yorsa (Jupyter, vb.) nest_asyncio gerekebilir
        try:
            loop = asyncio.get_running_loop()
            # Zaten bir event loop varsa, yeni thread'de Ã§alÄ±ÅŸtÄ±r
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                result = pool.submit(
                    asyncio.run,
                    self.synthesize(text, output_path, rate, volume, pitch)
                ).result()
            return result
        except RuntimeError:
            # Event loop yok â€” normal asyncio.run kullan
            return asyncio.run(
                self.synthesize(text, output_path, rate, volume, pitch)
            )

    @staticmethod
    async def list_voices(language: str = "tr") -> list[dict]:
        """
        KullanÄ±labilir sesleri listele.

        Args:
            language: Dil filtresi ("tr", "en", vb.)

        Returns:
            [{"name": "tr-TR-EmelNeural", "gender": "Female", "locale": "tr-TR"}]
        """
        import edge_tts

        voices = await edge_tts.list_voices()
        filtered = [
            {
                "name": v["ShortName"],
                "gender": v["Gender"],
                "locale": v["Locale"],
            }
            for v in voices
            if v["Locale"].startswith(language)
        ]
        return filtered


class VoiceAssistant:
    """
    BirleÅŸik Voice Assistant â€” ASR + Agent + TTS pipeline.

    Tam akÄ±ÅŸ:
    1. KullanÄ±cÄ± ses dosyasÄ± verir
    2. Whisper â†’ metin (ASR)
    3. Metin â†’ Agent graph â†’ cevap
    4. Cevap â†’ Edge-TTS â†’ ses dosyasÄ± (TTS)

    MÃ¼lakat notu:
    - End-to-end voice pipeline: ASR â†’ NLU â†’ Agent â†’ NLG â†’ TTS
    - Latency bileÅŸenleri: ASR (~1-3s) + Agent (~3-8s) + TTS (~1-2s) = ~5-13s
    - Real-time iÃ§in: streaming ASR (Whisper desteklemez) + streaming TTS
    - Production'da: WebSocket ile chunk-based streaming
    """

    def __init__(
        self,
        whisper_model: str = "base",
        tts_voice: str = "tr_female"
    ):
        self.asr = WhisperASR(model_size=whisper_model)
        self.tts = EdgeTTS(voice=tts_voice)

    def process_voice_query(
        self,
        audio_path: str,
        image_path: Optional[str] = None,
        output_audio_path: Optional[str] = None
    ) -> dict:
        """
        Ses giriÅŸini iÅŸle â†’ Agent'a gÃ¶nder â†’ Ses Ã§Ä±ktÄ±sÄ± Ã¼ret.

        Args:
            audio_path: GiriÅŸ ses dosyasÄ±
            image_path: Opsiyonel gÃ¶rÃ¼ntÃ¼ (multi-modal analiz iÃ§in)
            output_audio_path: TTS Ã§Ä±ktÄ± yolu (None â†’ otomatik)

        Returns:
            {
                "transcription": {...},
                "agent_response": "...",
                "tts_output": {...}
            }
        """
        # 1. ASR â€” Ses â†’ Metin
        print(f"\n{'='*60}")
        print(f"ğŸ¤ VOICE ASSISTANT â€” Processing")
        print(f"{'='*60}")

        transcription = self.asr.transcribe(audio_path, language="tr")
        if "error" in transcription:
            return {"error": transcription["error"]}

        user_query = transcription["text"]
        print(f"ğŸ“ Transkript: \"{user_query}\"")

        # 2. Agent â€” Metin â†’ Cevap
        from .state import create_initial_state
        from .graph import build_agent_graph

        state = create_initial_state(
            user_query=user_query,
            image_path=image_path,
            max_iterations=3
        )

        graph = build_agent_graph(with_memory=False)
        result = graph.invoke(state)
        agent_answer = result.get("final_answer", "Cevap Ã¼retilemedi.")
        print(f"ğŸ’¬ Agent cevabÄ±: \"{agent_answer[:200]}...\"")

        # 3. TTS â€” Cevap â†’ Ses
        if output_audio_path is None:
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                "output"
            )
            os.makedirs(output_dir, exist_ok=True)
            output_audio_path = os.path.join(output_dir, "response.mp3")

        # Sadece ana cevabÄ± seslendir (metadata kÄ±smÄ±nÄ± deÄŸil)
        # Ä°lk satÄ±rÄ± al (ğŸ“ ve ğŸ“Š satÄ±rlarÄ±nÄ± atla)
        clean_answer = agent_answer.split("\nğŸ“")[0].split("\nğŸ“Š")[0].strip()

        tts_result = self.tts.synthesize_sync(clean_answer, output_audio_path)
        print(f"ğŸ”Š TTS Ã§Ä±ktÄ±sÄ±: {output_audio_path}")

        return {
            "transcription": transcription,
            "user_query": user_query,
            "agent_response": agent_answer,
            "tts_output": tts_result
        }
