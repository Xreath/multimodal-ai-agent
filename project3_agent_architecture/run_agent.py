"""
Multi-Modal Agent CLI â€” LangGraph tabanlÄ± agent'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.

KullanÄ±m:
    # GÃ¶rÃ¼ntÃ¼ analizi (CV pipeline + LLM reasoning)
    python run_agent.py analyze --image ../project1_cv_pipeline/data/bus.jpg --query "Bu sahnede ne var?"

    # Sadece reasoning (gÃ¶rÃ¼ntÃ¼sÃ¼z)
    python run_agent.py ask --query "YOLO'nun Ã§alÄ±ÅŸma prensibi nedir?"

    # Interactive mod (multi-turn konuÅŸma)
    python run_agent.py interactive --image ../project1_cv_pipeline/data/bus.jpg

    # Graph yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtir
    python run_agent.py graph

    # Memory demo (vector store ile hatÄ±rlama)
    python run_agent.py memory-demo
"""

import argparse
import json
import os
import sys

# Proje path'lerini ekle
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MONOREPO_ROOT = os.path.dirname(PROJECT_ROOT)
sys.path.insert(0, PROJECT_ROOT)

from src.state import create_initial_state
from src.graph import build_agent_graph, visualize_graph


def run_analyze(args):
    """
    Tek seferlik analiz modu.

    AkÄ±ÅŸ:
    1. State oluÅŸtur (query + image)
    2. Graph'Ä± derle
    3. Ã‡alÄ±ÅŸtÄ±r (planner â†’ router â†’ vision â†’ reasoner â†’ evaluator â†’ respond)
    4. Sonucu gÃ¶ster
    """
    print("\n" + "=" * 70)
    print("ğŸ¤– MULTI-MODAL AGENT â€” Analyze Mode")
    print("=" * 70)

    # Initial state
    state = create_initial_state(
        user_query=args.query,
        image_path=args.image,
        max_iterations=args.max_iter
    )

    # Graph compile & run
    graph = build_agent_graph(with_memory=False)

    print(f"\nSorgu: {args.query}")
    if args.image:
        print(f"GÃ¶rÃ¼ntÃ¼: {args.image}")
    print(f"Max iteration: {args.max_iter}")
    print("-" * 70)

    # Invoke â€” graph'Ä± baÅŸtan sona Ã§alÄ±ÅŸtÄ±r
    result = graph.invoke(state)

    # SonuÃ§
    print("\n" + "=" * 70)
    print("ğŸ“‹ SONUÃ‡")
    print("=" * 70)
    print(result.get("final_answer", "Cevap Ã¼retilemedi."))

    # DetaylarÄ± kaydet
    if args.output:
        output_data = {
            "query": args.query,
            "image_path": args.image,
            "plan": result.get("plan", []),
            "final_answer": result.get("final_answer"),
            "evaluation_score": result.get("evaluation_score"),
            "evaluation_feedback": result.get("evaluation_feedback"),
            "tool_results": result.get("tool_results", []),
            "iteration_count": result.get("iteration_count"),
            "messages": result.get("messages", []),
        }
        os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ SonuÃ§ kaydedildi: {args.output}")


def run_ask(args):
    """GÃ¶rÃ¼ntÃ¼sÃ¼z soru-cevap modu."""
    print("\n" + "=" * 70)
    print("ğŸ¤– MULTI-MODAL AGENT â€” Ask Mode (no image)")
    print("=" * 70)

    state = create_initial_state(
        user_query=args.query,
        image_path=None,
        max_iterations=args.max_iter
    )

    graph = build_agent_graph(with_memory=False)
    result = graph.invoke(state)

    print("\n" + "=" * 70)
    print("ğŸ“‹ SONUÃ‡")
    print("=" * 70)
    print(result.get("final_answer", "Cevap Ã¼retilemedi."))


def run_interactive(args):
    """
    Interactive mod â€” multi-turn konuÅŸma.

    Her turda:
    1. KullanÄ±cÄ±dan soru al
    2. Agent graph'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r
    3. CevabÄ± gÃ¶ster
    4. Memory'e kaydet (long-term)
    5. Tekrarla

    MÃ¼lakat notu:
    - Multi-turn state: her turda graph yeniden Ã§alÄ±ÅŸÄ±r
    - Memory persistence: geÃ§miÅŸ analizler hatÄ±rlanÄ±r
    - 'q' ile Ã§Ä±kÄ±ÅŸ
    """
    from src.memory import MemoryManager

    print("\n" + "=" * 70)
    print("ğŸ¤– MULTI-MODAL AGENT â€” Interactive Mode")
    print("=" * 70)
    print("Komutlar: 'q' â†’ Ã§Ä±kÄ±ÅŸ, 'memory' â†’ hafÄ±zadaki kayÄ±tlar")

    if args.image:
        print(f"GÃ¶rÃ¼ntÃ¼: {args.image}")

    memory = MemoryManager(
        persist_dir=os.path.join(PROJECT_ROOT, "data", "memory")
    )

    turn = 0
    while True:
        turn += 1
        try:
            query = input(f"\n[Turn {turn}] Soru: ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if not query:
            continue
        if query.lower() in ("q", "quit", "exit"):
            print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
        if query.lower() == "memory":
            print(f"ğŸ“š HafÄ±zada {memory.vector_store.count} kayÄ±t var")
            if memory.vector_store.count > 0:
                recent = memory.vector_store.search("son analiz", n_results=3)
                for i, mem in enumerate(recent, 1):
                    print(f"  {i}. {mem['text'][:200]}...")
            continue

        # Memory'den ilgili context al
        relevant_context = memory.get_full_context(query)
        full_query = query
        if relevant_context:
            full_query = f"{query}\n\nGeÃ§miÅŸ bilgi:\n{relevant_context}"

        # Agent Ã§alÄ±ÅŸtÄ±r
        state = create_initial_state(
            user_query=full_query,
            image_path=args.image if turn == 1 or args.image else None,
            max_iterations=args.max_iter
        )

        graph = build_agent_graph(with_memory=False)
        result = graph.invoke(state)

        answer = result.get("final_answer", "Cevap Ã¼retilemedi.")
        print(f"\nğŸ’¬ Cevap:\n{answer}")

        # Memory'e kaydet
        memory.add_conversation_message("user", query)
        memory.add_conversation_message("assistant", answer)
        memory.store_analysis(
            f"Soru: {query}\nCevap: {answer[:500]}",
            metadata={"turn": turn, "has_image": bool(args.image)}
        )


def run_graph_viz(args):
    """Graph yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtir."""
    print("\n" + "=" * 70)
    print("ğŸ“Š AGENT GRAPH VISUALIZATION")
    print("=" * 70)

    graph = build_agent_graph(with_memory=False)
    mermaid = visualize_graph(graph)
    print(mermaid)

    # Mermaid dosyasÄ± kaydet
    output_path = os.path.join(PROJECT_ROOT, "output", "graph.mmd")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        f.write(mermaid)
    print(f"\nğŸ’¾ Mermaid diagram kaydedildi: {output_path}")
    print("GÃ¶rselleÅŸtirmek iÃ§in: https://mermaid.live/")


def run_memory_demo(args):
    """
    Memory demo â€” Vector store'un nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.

    Bu demo:
    1. BirkaÃ§ Ã¶rnek analiz sonucunu vector store'a kaydet
    2. Semantic search ile ilgili kayÄ±tlarÄ± bul
    3. RAG-style context oluÅŸtur
    """
    from src.memory import MemoryManager

    print("\n" + "=" * 70)
    print("ğŸ§  MEMORY DEMO â€” Vector Store")
    print("=" * 70)

    memory = MemoryManager(
        persist_dir=os.path.join(PROJECT_ROOT, "data", "memory_demo")
    )

    # Ã–rnek analizleri kaydet
    analyses = [
        ("OtobÃ¼s fotoÄŸrafÄ±nda 4 kiÅŸi, 1 otobÃ¼s, 2 araba tespit edildi. "
         "Plaka numarasÄ±: 34 ABC 123. Sahne bir ÅŸehir sokaÄŸÄ±.",
         {"type": "vehicle_detection", "image": "bus.jpg"}),

        ("Depo gÃ¶rÃ¼ntÃ¼sÃ¼nde 3 iÅŸÃ§i baret takmÄ±yor. Forklift geÃ§iÅŸ alanÄ±nda "
         "engel var. 2 gÃ¼venlik ihlali tespit edildi.",
         {"type": "safety_inspection", "image": "warehouse.jpg"}),

        ("Otopark gÃ¶rÃ¼ntÃ¼sÃ¼nde 12 araÃ§ var. 3'Ã¼ kÄ±rmÄ±zÄ±, 5'i beyaz, 4'Ã¼ siyah. "
         "BoÅŸ park yeri sayÄ±sÄ±: 8. Doluluk oranÄ±: %60.",
         {"type": "parking_analysis", "image": "parking.jpg"}),

        ("Trafik kamerasÄ±: saat 08:30'da yoÄŸun trafik. 45 araÃ§/dakika geÃ§iÅŸ. "
         "KÄ±rmÄ±zÄ± Ä±ÅŸÄ±k ihlali: 2 araÃ§. Ortalama hÄ±z: 25 km/h.",
         {"type": "traffic_analysis", "image": "traffic_cam.jpg"}),
    ]

    print("\nğŸ“ Analizler kaydediliyor...")
    for text, metadata in analyses:
        memory.store_analysis(text, metadata)
        print(f"  âœ… {metadata['type']} ({metadata['image']})")

    print(f"\nğŸ“š Toplam kayÄ±t: {memory.vector_store.count}")

    # Semantic search demo
    queries = [
        "araÃ§lar ve park yeri",
        "gÃ¼venlik ihlali var mÄ±?",
        "trafik yoÄŸunluÄŸu nasÄ±l?",
    ]

    for query in queries:
        print(f"\nğŸ” Arama: \"{query}\"")
        results = memory.vector_store.search(query, n_results=2)
        for i, r in enumerate(results, 1):
            distance = f"{r['distance']:.4f}" if r['distance'] is not None else "N/A"
            print(f"  {i}. [mesafe: {distance}] {r['text'][:120]}...")

    # RAG context demo
    print(f"\n{'='*60}")
    print("ğŸ“„ RAG Context Ã¶rneÄŸi:")
    print("=" * 60)
    context = memory.get_full_context("depo gÃ¼venliÄŸi")
    print(context)


def run_tts_demo(args):
    """
    TTS Demo â€” Metni sese Ã§evirir (Edge-TTS).

    Edge-TTS Microsoft'un neural TTS motorunu kullanÄ±r:
    - Ãœcretsiz, API key gerektirmez
    - 300+ ses, 80+ dil
    - TÃ¼rkÃ§e: EmelNeural (kadÄ±n), AhmetNeural (erkek)
    """
    from src.voice import EdgeTTS

    print("\n" + "=" * 70)
    print("ğŸ”Š TTS DEMO â€” Edge-TTS")
    print("=" * 70)

    tts = EdgeTTS(voice=args.voice)
    output_path = args.output or os.path.join(PROJECT_ROOT, "output", "tts_output.mp3")

    print(f"Metin: \"{args.text}\"")
    print(f"Ses: {tts.voice}")
    print(f"HÄ±z: {args.rate}")

    result = tts.synthesize_sync(args.text, output_path, rate=args.rate)

    print(f"\nâœ… Ses dosyasÄ± oluÅŸturuldu: {result['output_path']}")
    print(f"   Kelime sayÄ±sÄ±: {result['word_count']}")
    print(f"   Tahmini sÃ¼re: {result['duration_estimate_seconds']}s")
    print(f"\nâ–¶ï¸  Dinlemek iÃ§in: open {result['output_path']}")


def run_asr_demo(args):
    """
    ASR Demo â€” Ses dosyasÄ±nÄ± metne Ã§evirir (Whisper).

    Whisper OpenAI'Ä±n ses tanÄ±ma modeli:
    - Offline Ã§alÄ±ÅŸÄ±r (API gerekmez)
    - 100+ dil desteÄŸi
    - Zaman damgalÄ± Ã§Ä±ktÄ± (altyazÄ± iÃ§in)
    """
    from src.voice import WhisperASR

    print("\n" + "=" * 70)
    print("ğŸ¤ ASR DEMO â€” Whisper")
    print("=" * 70)

    asr = WhisperASR(model_size=args.model)

    if args.detect_language:
        print(f"Dil tespiti: {args.audio}")
        result = asr.detect_language(args.audio)
        print(f"Tespit edilen dil: {result['detected_language']} "
              f"(confidence: {result['confidence']:.2%})")
        print(f"Top 5: {result['top_5']}")
        return

    print(f"Ses dosyasÄ±: {args.audio}")
    print(f"Model: {args.model}")
    lang_info = f", dil: {args.language}" if args.language else ", dil: auto-detect"
    print(f"Ayarlar{lang_info}")

    result = asr.transcribe(
        args.audio,
        language=args.language,
        task=args.task
    )

    if "error" in result:
        print(f"âŒ Hata: {result['error']}")
        return

    print(f"\n{'='*60}")
    print(f"ğŸ“ TRANSKRIPT")
    print(f"{'='*60}")
    print(result["text"])
    print(f"\nDil: {result['language']}")
    print(f"SÃ¼re: {result['duration']}s")
    print(f"Segment sayÄ±sÄ±: {len(result['segments'])}")

    if args.segments:
        print(f"\nğŸ“‹ Segmentler (zaman damgalÄ±):")
        for seg in result["segments"]:
            print(f"  [{seg['start']:>6.2f}s â†’ {seg['end']:>6.2f}s] {seg['text']}")

    # JSON kaydet
    if args.output:
        import json
        os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ SonuÃ§ kaydedildi: {args.output}")


def run_voice_pipeline(args):
    """
    Voice Pipeline â€” Tam akÄ±ÅŸ: Ses â†’ ASR â†’ Agent â†’ TTS â†’ Ses.

    End-to-end voice assistant:
    1. KullanÄ±cÄ± ses dosyasÄ± verir
    2. Whisper metin Ã§Ä±karÄ±r
    3. Agent graph cevap Ã¼retir
    4. Edge-TTS cevabÄ± seslendirir
    """
    from src.voice import VoiceAssistant

    print("\n" + "=" * 70)
    print("ğŸ™ï¸  VOICE PIPELINE â€” Ses â†’ Agent â†’ Ses")
    print("=" * 70)

    assistant = VoiceAssistant(
        whisper_model=args.whisper_model,
        tts_voice=args.tts_voice
    )

    result = assistant.process_voice_query(
        audio_path=args.audio,
        image_path=args.image,
        output_audio_path=args.output
    )

    if "error" in result:
        print(f"âŒ Hata: {result['error']}")
        return

    print(f"\n{'='*60}")
    print(f"ğŸ“‹ SONUÃ‡LAR")
    print(f"{'='*60}")
    print(f"ğŸ¤ Transkript: \"{result['user_query']}\"")
    print(f"ğŸ’¬ Agent cevabÄ±:\n{result['agent_response']}")
    print(f"ğŸ”Š Ses Ã§Ä±ktÄ±sÄ±: {result['tts_output']['output_path']}")
    print(f"\nâ–¶ï¸  Dinlemek iÃ§in: open {result['tts_output']['output_path']}")


def main():
    parser = argparse.ArgumentParser(
        description="Multi-Modal Agent â€” LangGraph tabanlÄ± agentic sistem",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python run_agent.py analyze --image ../project1_cv_pipeline/data/bus.jpg -q "Bu sahnede ne var?"
  python run_agent.py ask -q "Object detection nasÄ±l Ã§alÄ±ÅŸÄ±r?"
  python run_agent.py interactive --image ../project1_cv_pipeline/data/bus.jpg
  python run_agent.py graph
  python run_agent.py memory-demo
  python run_agent.py tts --text "Merhaba, ben bir AI asistanÄ±yÄ±m."
  python run_agent.py asr --audio ses.wav
  python run_agent.py voice --audio soru.wav --image bus.jpg
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="Ã‡alÄ±ÅŸma modu")

    # analyze
    p_analyze = subparsers.add_parser("analyze", help="GÃ¶rÃ¼ntÃ¼ + soru analizi")
    p_analyze.add_argument("--image", "-i", help="GÃ¶rÃ¼ntÃ¼ dosya yolu")
    p_analyze.add_argument("--query", "-q", required=True, help="Soru")
    p_analyze.add_argument("--output", "-o", default="output/agent_result.json", help="Ã‡Ä±ktÄ± JSON yolu")
    p_analyze.add_argument("--max-iter", type=int, default=5, help="Max iteration sayÄ±sÄ±")

    # ask
    p_ask = subparsers.add_parser("ask", help="GÃ¶rÃ¼ntÃ¼sÃ¼z soru-cevap")
    p_ask.add_argument("--query", "-q", required=True, help="Soru")
    p_ask.add_argument("--max-iter", type=int, default=3, help="Max iteration sayÄ±sÄ±")

    # interactive
    p_inter = subparsers.add_parser("interactive", help="Interactive multi-turn mod")
    p_inter.add_argument("--image", "-i", help="GÃ¶rÃ¼ntÃ¼ dosya yolu")
    p_inter.add_argument("--max-iter", type=int, default=5, help="Max iteration sayÄ±sÄ±")

    # graph
    subparsers.add_parser("graph", help="Graph yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtir")

    # memory-demo
    subparsers.add_parser("memory-demo", help="Memory (vector store) demo")

    # tts â€” Text-to-Speech
    p_tts = subparsers.add_parser("tts", help="Metin â†’ Ses (Edge-TTS)")
    p_tts.add_argument("--text", "-t", required=True, help="Seslendirilecek metin")
    p_tts.add_argument("--voice", "-v", default="tr_female",
                       help="Ses: tr_female, tr_male, en_female, en_male (varsayÄ±lan: tr_female)")
    p_tts.add_argument("--rate", "-r", default="+0%", help="HÄ±z: '+20%%' daha hÄ±zlÄ±, '-20%%' daha yavaÅŸ")
    p_tts.add_argument("--output", "-o", help="Ã‡Ä±ktÄ± dosya yolu (.mp3)")

    # asr â€” Speech-to-Text
    p_asr = subparsers.add_parser("asr", help="Ses â†’ Metin (Whisper)")
    p_asr.add_argument("--audio", "-a", required=True, help="Ses dosyasÄ± yolu")
    p_asr.add_argument("--model", "-m", default="base",
                       help="Whisper model: tiny, base, small, medium, large (varsayÄ±lan: base)")
    p_asr.add_argument("--language", "-l", help="Dil kodu: tr, en, vb. (varsayÄ±lan: auto-detect)")
    p_asr.add_argument("--task", default="transcribe",
                       help="transcribe (aynÄ± dil) veya translate (Ä°ngilizce'ye Ã§evir)")
    p_asr.add_argument("--segments", "-s", action="store_true", help="Zaman damgalÄ± segmentleri gÃ¶ster")
    p_asr.add_argument("--detect-language", action="store_true", help="Sadece dil tespiti yap")
    p_asr.add_argument("--output", "-o", help="JSON Ã§Ä±ktÄ± yolu")

    # voice â€” Full voice pipeline
    p_voice = subparsers.add_parser("voice", help="Ses â†’ Agent â†’ Ses (tam pipeline)")
    p_voice.add_argument("--audio", "-a", required=True, help="GiriÅŸ ses dosyasÄ±")
    p_voice.add_argument("--image", "-i", help="Opsiyonel gÃ¶rÃ¼ntÃ¼ (multi-modal analiz)")
    p_voice.add_argument("--whisper-model", default="base", help="Whisper model boyutu")
    p_voice.add_argument("--tts-voice", default="tr_female", help="TTS ses seÃ§imi")
    p_voice.add_argument("--output", "-o", help="TTS Ã§Ä±ktÄ± dosya yolu (.mp3)")

    args = parser.parse_args()

    if args.command == "analyze":
        run_analyze(args)
    elif args.command == "ask":
        run_ask(args)
    elif args.command == "interactive":
        run_interactive(args)
    elif args.command == "graph":
        run_graph_viz(args)
    elif args.command == "memory-demo":
        run_memory_demo(args)
    elif args.command == "tts":
        run_tts_demo(args)
    elif args.command == "asr":
        run_asr_demo(args)
    elif args.command == "voice":
        run_voice_pipeline(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
